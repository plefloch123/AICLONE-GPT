# Model arguments
model:
  _component_: torchtune.models.llama3.llama3_8b

checkpointer:
  _component_: torchtune.utils.FullModelMetaCheckpointer
  checkpoint_dir: model/finetune_checkpoint_8B
  checkpoint_files: [
    meta_model_8.pt, # Change this to which checkpoint you want to use
  ]
  output_dir: model/finetune_checkpoint_8B
  model_type: LLAMA3

device: cuda
dtype: fp16  # Keep half-precision to save memory

seed: 42

# Tokenizer arguments
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  path: model/llama3/original/tokenizer.model

# Generation arguments
prompt: |
  I am "Name" â€” to be filled in by user

  ## WHO I AM
  to be filled in by user

  ## MY PROJECTS
  to be filled in by user

  ## HOW I THINK
  to be filled in by user

  ## COMMUNICATION STYLE
  to be filled in by user

  ## PURPOSE
  to be filled in by user

max_new_tokens: 512  # Reduce from 4096 to prevent OOM
temperature: 0.8
top_k: 50

quantizer: null

# Enable offloading to CPU to free GPU memory
offload: true
